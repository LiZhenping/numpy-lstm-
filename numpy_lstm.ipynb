{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "numpy_lstm (4).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "fHaTpMLdneR2",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHdNs60yneR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import tensorflow as tf\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9uQaBkFneR8",
        "colab_type": "text"
      },
      "source": [
        "### Read and process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1llWj0rlneR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea22f436-842f-4ef0-8b77-018643f3feb4"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "data = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWu37n8hneR_",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRVOUeB4neR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c74c9cdc-a6c6-4af3-d725-d8b849ab0a23"
      },
      "source": [
        "chars = sorted(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehqslELCneSC",
        "colab_type": "text"
      },
      "source": [
        "### Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo2xG8t2neSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_size = 100 # Size of the hidden layer\n",
        "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning rate\n",
        "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
        "z_size = H_size + X_size # Size of concatenate(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrRtkFEfneSF",
        "colab_type": "text"
      },
      "source": [
        "### Activation Functions and Derivatives\n",
        "\n",
        "#### Sigmoid\n",
        "\n",
        "\\begin{align}\n",
        "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
        "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "\\end{align}\n",
        "\n",
        "#### Tanh\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlf9UoN9neSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def dsigmoid(y):\n",
        "    return y * (1 - y)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def dtanh(y):\n",
        "    return 1 - y * y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRl-xLIuneSI",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlLb9aXoneSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "        self.name = name\n",
        "        self.v = value #parameter value\n",
        "        self.d = np.zeros_like(value) #derivative\n",
        "        self.m = np.zeros_like(value) #momentum for AdaGrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsdK5Iy6neSL",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnrLvTwkneSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', \n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd)\n",
        "        self.b_C = Param('b_C',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o',\n",
        "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o',\n",
        "                         np.zeros((H_size, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v',\n",
        "                         np.random.randn(X_size, H_size) * weight_sd)\n",
        "        self.b_v = Param('b_v',\n",
        "                         np.zeros((X_size, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g-TnQqJneSO",
        "colab_type": "text"
      },
      "source": [
        "### Forward pass\n",
        "\n",
        "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
        "\n",
        "#### Concatenation of $h_{t-1}$ and $x_t$\n",
        "\\begin{align}\n",
        "z & = [h_{t-1}, x_t] \\\\\n",
        "\\end{align}\n",
        "\n",
        "#### LSTM functions\n",
        "\\begin{align}\n",
        "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
        "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
        "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
        "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
        "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
        "h_t &= o_t * tanh(C_t) \\\\\n",
        "\\end{align}\n",
        "\n",
        "#### Logits\n",
        "\\begin{align}\n",
        "v_t &= W_v \\cdot h_t + b_v \\\\\n",
        "\\end{align}\n",
        "\n",
        "#### Softmax\n",
        "\\begin{align}\n",
        "\\hat{y_t} &= \\text{softmax}(v_t)\n",
        "\\end{align}\n",
        "\n",
        "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVO7vyUxneSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (H_size, 1)\n",
        "    assert C_prev.shape == (H_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HkGYNcNneSR",
        "colab_type": "text"
      },
      "source": [
        "### Backward pass\n",
        "\n",
        "#### Loss\n",
        "\n",
        "\\begin{align}\n",
        "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
        "L &= L_1 \\\\\n",
        "\\end{align}\n",
        "\n",
        "#### Gradients\n",
        "\n",
        "\\begin{align}\n",
        "dv_t &= \\hat{y_t} - y_t \\\\\n",
        "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
        "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
        "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
        "d\\bar{C}_t &= dC_t * i_t \\\\\n",
        "di_t &= dC_t * \\bar{C}_t \\\\\n",
        "df_t &= dC_t * C_{t-1} \\\\\n",
        "\\\\\n",
        "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
        "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
        "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
        "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
        "dz_t &= W_f^T \\cdot df'_t \\\\\n",
        "     &+ W_i^T \\cdot di_t \\\\\n",
        "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
        "     &+ W_o^T \\cdot do_t \\\\\n",
        "\\\\\n",
        "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
        "dC'_t &= f_t * dC_t\n",
        "\\end{align}\n",
        "\n",
        "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
        "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
        "* All other derivatives are of $L$\n",
        "* `target` is target character index $y_t$\n",
        "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
        "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
        "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
        "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
        "* *Returns* $dh_t$ and $dC_t$\n",
        "\n",
        "#### Model parameter gradients\n",
        "\n",
        "\\begin{align}\n",
        "dW_v &= dv_t \\cdot h_t^T \\\\\n",
        "db_v &= dv_t \\\\\n",
        "\\\\\n",
        "dW_f &= df'_t \\cdot z^T \\\\\n",
        "db_f &= df'_t \\\\\n",
        "\\\\\n",
        "dW_i &= di'_t \\cdot z^T \\\\\n",
        "db_i &= di'_t \\\\\n",
        "\\\\\n",
        "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
        "db_C &= d\\bar{C}'_t \\\\\n",
        "\\\\\n",
        "dW_o &= do'_t \\cdot z^T \\\\\n",
        "db_o &= do'_t \\\\\n",
        "\\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcvyNawfneSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + H_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (H_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:H_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcky8BdhneSU",
        "colab_type": "text"
      },
      "source": [
        "### Forward Backward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17NGSWN1neSU",
        "colab_type": "text"
      },
      "source": [
        "Clear gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDUlTIumneSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-VzhZT6neSX",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG259JmCneSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPxzNNWcneSa",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "* `input`, `target` are list of integers, with character indexes.\n",
        "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
        "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
        "* *Returns* loss, final $h_T$ and $C_T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ9iK0Y4neSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == T_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jJm7OtDneSd",
        "colab_type": "text"
      },
      "source": [
        "### Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzJtZBQyneSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Onk7HRTneSg",
        "colab_type": "text"
      },
      "source": [
        "## Training (Adagrad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu5COYPyneSh",
        "colab_type": "text"
      },
      "source": [
        "Update the graph and display a sample output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGof2rpnneSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkalaiVwneSj",
        "colab_type": "text"
      },
      "source": [
        "Update parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7icZyq68neSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsd6RvNfneSn",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training \n",
        "from stopping in the middle of an iteration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGPwT3qPneSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import signal\n",
        "\n",
        "class DelayedKeyboardInterrupt(object):\n",
        "    def __enter__(self):\n",
        "        self.signal_received = False\n",
        "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
        "\n",
        "    def handler(self, sig, frame):\n",
        "        self.signal_received = (sig, frame)\n",
        "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        signal.signal(signal.SIGINT, self.old_handler)\n",
        "        if self.signal_received:\n",
        "            self.old_handler(*self.signal_received)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWzRfYTJneSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMPYdzKYneSt",
        "colab_type": "text"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYlom_ineSu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "6b5b70b5-b138-414e-f8e2-77886dae7877"
      },
      "source": [
        "while True:\n",
        "    try:\n",
        "        with DelayedKeyboardInterrupt():\n",
        "            # Reset\n",
        "            if pointer + T_steps >= len(data) or iteration == 0:\n",
        "                g_h_prev = np.zeros((H_size, 1))\n",
        "                g_C_prev = np.zeros((H_size, 1))\n",
        "                pointer = 0\n",
        "\n",
        "\n",
        "            inputs = ([char_to_idx[ch] \n",
        "                       for ch in data[pointer: pointer + T_steps]])\n",
        "            targets = ([char_to_idx[ch] \n",
        "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
        "\n",
        "            loss, g_h_prev, g_C_prev = \\\n",
        "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "            # Print every hundred steps\n",
        "            if iteration % 100 == 0:\n",
        "                update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "            update_paramters()\n",
        "\n",
        "            plot_iter = np.append(plot_iter, [iteration])\n",
        "            plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "            pointer += T_steps\n",
        "            iteration += 1\n",
        "    except KeyboardInterrupt:\n",
        "        update_status(inputs, g_h_prev, g_C_prev)\n",
        "        break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEDKGRJwgCELG/D/6Td97Elvp0wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " mt' thof Abt ber paisthot he oaran res\n",
            "Tow mdeouls\n",
            "vNge\n",
            "Thafbom. Ahaen tils taam hou,, Patcrt yeu hatstc\n",
            "\n",
            "thon fa ulc, li:\n",
            "War ards aad cl lds au, em: dith Iuzosn\n",
            "Ad you senlyovde Vket covs harsns cig \n",
            "----\n",
            "iter 871, loss 85.646556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbPwwVM5neSx",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Check\n",
        "\n",
        "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
        "\n",
        "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxjIx3mHneSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF4JHQ1pneSz",
        "colab_type": "text"
      },
      "source": [
        "Calculate numerical gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCiQDJfkneS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
        "    old_val = param.v.flat[idx]\n",
        "    \n",
        "    # evaluate loss at [x + delta] and [x - delta]\n",
        "    param.v.flat[idx] = old_val + delta\n",
        "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
        "                                             h_prev, C_prev)\n",
        "    param.v.flat[idx] = old_val - delta\n",
        "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
        "                                             h_prev, C_prev)\n",
        "    \n",
        "    param.v.flat[idx] = old_val #reset\n",
        "\n",
        "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
        "    # Clip numerical error because analytical gradient is clipped\n",
        "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
        "    \n",
        "    return grad_numerical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT_4w49MneS4",
        "colab_type": "text"
      },
      "source": [
        "Check gradient of each paramter matrix/vector at `num_checks` individual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VCfTpcKneS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
        "    global parameters\n",
        "    \n",
        "    # To calculate computed gradients\n",
        "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
        "    \n",
        "    \n",
        "    for param in parameters.all():\n",
        "        #Make a copy because this will get modified\n",
        "        d_copy = np.copy(param.d)\n",
        "\n",
        "        # Test num_checks times\n",
        "        for i in range(num_checks):\n",
        "            # Pick a random index\n",
        "            rnd_idx = int(uniform(0, param.v.size))\n",
        "            \n",
        "            grad_numerical = calc_numerical_gradient(param,\n",
        "                                                     rnd_idx,\n",
        "                                                     delta,\n",
        "                                                     inputs,\n",
        "                                                     target,\n",
        "                                                     h_prev, C_prev)\n",
        "            grad_analytical = d_copy.flat[rnd_idx]\n",
        "\n",
        "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
        "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
        "            \n",
        "            # If relative error is greater than 1e-06\n",
        "            if rel_error > 1e-06:\n",
        "                print('%s (%e, %e) => %e'\n",
        "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RakAtQyDneS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}